#!/usr/bin/env python

"""
Test influx throughput by pregenerating a bunch of dummy data and submitting it as fast as possible.
"""

import logging
import textwrap
from datetime import datetime
from argparse import ArgumentParser, Namespace

from influxdb_client import InfluxDBClient, Point, WriteOptions


def parse_args() -> Namespace:
    parser = ArgumentParser()

    parser.add_argument('--token', type=str, required=True)
    parser.add_argument('--url', type=str, default='http://localhost:8086')
    parser.add_argument('--bucket', type=str, default='testbkt')
    parser.add_argument('--org', type=str, default='testorg')
    parser.add_argument('--count', type=int, default='3000000')
    parser.add_argument('--batch', type=int, default='50000')
    parser.add_argument('--flush', type=float, default='15')

    return parser.parse_args()


def genpoints(n):
    return [Point('throughput_test').field('x', i) for i in range(n)]


def main():
    logging.basicConfig(level=logging.DEBUG)
    args = parse_args()

    logging.debug('generating data... ')
    data = genpoints(args.count)
    logging.debug('data generated')

    with InfluxDBClient(args.url, token=args.token) as client:
        with client.write_api(write_options=WriteOptions(
            batch_size=args.batch,
            flush_interval=int(args.flush * 1000),
            jitter_interval=3_000,
        )) as wapi:
            logging.info('connected, writing')
            start = datetime.now()

            wapi.write(args.bucket, args.org, data)

    diff = datetime.now() - start
    logging.info(textwrap.dedent(f'''
    done. stats:
        elapsed: {diff}
        per_item: {diff/args.count}
        throughput: {args.count/diff.total_seconds()} it/s
    '''.strip()))


if __name__ == '__main__':
    main()
